# ============================================
# RAG CLI Application - Environment Variables
# ============================================
# Copy this file to .env and fill in your values
# cp env.example .env

# ============================================
# LLM Provider Selection
# ============================================
# Choose your LLM provider: "gemini" or "ollama"
# - gemini: Uses Google Gemini API (requires API key)
# - ollama: Uses local Ollama models (requires Ollama running locally)
LLM_PROVIDER=gemini

# ============================================
# Google Gemini API Configuration
# ============================================
# Required only if LLM_PROVIDER=gemini
# Get your API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# ============================================
# Redis Configuration
# ============================================
# Redis connection URL
# 
# When using docker-compose (recommended):
#   - Container name: redis-stack
#   - Port: 6379 (default Redis port)
#   - Use: redis://localhost:6379
#
# When using Docker directly:
#   - If using default ports: redis://localhost:6379
#   - If using custom ports: redis://localhost:YOUR_PORT
#
# For Redis Cloud or remote Redis:
#   - Format: redis://username:password@host:port
#   - Example: redis://myuser:mypass@redis.example.com:6379
#
# For Redis with authentication (if configured):
#   - Format: redis://:password@localhost:6379
#   - Or: redis://username:password@localhost:6379
REDIS_URL=redis://localhost:6379

# ============================================
# Gemini Model Configuration (Optional)
# ============================================
# Required only if LLM_PROVIDER=gemini
# Model for text generation
# Options: gemini-pro, gemini-1.5-pro, gemini-1.5-flash, etc.
GEMINI_MODEL=gemini-pro

# Embedding model for vector embeddings
# Note: embedding-001 produces 768-dimensional vectors
GEMINI_EMBEDDING_MODEL=models/embedding-001

# ============================================
# Ollama Configuration (Optional)
# ============================================
# Required only if LLM_PROVIDER=ollama
# Base URL for Ollama API (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model for text generation
# Recommended models:
#   - llama3.1:8b (good balance of quality and speed)
#   - mistral (excellent quality, slightly slower)
#   - llama3:8b (fast, good quality)
#   - gemma2:9b (Google's open model)
OLLAMA_MODEL=llama3.1:8b

# Ollama embedding model
# Recommended: nomic-embed-text (768 dimensions, optimized for semantic search)
# Other options: mxbai-embed-large, all-minilm, etc.
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# ============================================
# Semantic Cache Configuration (Optional)
# ============================================
# Similarity threshold for cache hits (0.0 to 1.0)
# - Higher values (e.g., 0.95) = stricter matching, fewer cache hits
# - Lower values (e.g., 0.8) = more lenient matching, more cache hits
# - Default: 0.9 (90% similarity required)
CACHE_SIMILARITY_THRESHOLD=0.9


# ============================================
# Advanced Configuration (Optional)
# ============================================
# Vector dimension for embeddings
# Default: 768 (for Gemini embedding-001)
# Only change if using a different embedding model
# VECTOR_DIM=768

